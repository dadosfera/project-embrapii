{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5752676e",
   "metadata": {},
   "source": [
    "# An√°lise de Leitos e Hospitais (DataSUS)\n",
    "\n",
    "---\n",
    "\n",
    "**Fonte dos Dados:** [DataSUS - Hospitais e Leitos](https://opendatasus.saude.gov.br/dataset/hospitais-e-leitos)\n",
    "\n",
    "### Descri√ß√£o do Dataset\n",
    "Este conjunto de dados apresenta informa√ß√µes detalhadas sobre a capacidade hospitalar no Brasil, extra√≠da do Cadastro Nacional de Estabelecimentos de Sa√∫de (CNES). O foco principal √© o monitoramento da quantidade de **leitos existentes** e **leitos SUS** (incluindo UTIs de diversas especialidades) ao longo do tempo.\n",
    "\n",
    "A an√°lise abaixo realiza um processo de ETL (Extra√ß√£o, Transforma√ß√£o e Carga) automatizado que:\n",
    "1.  Baixa os dados brutos do DataSUS.\n",
    "2.  Normaliza e limpa os dados (tratamento de datas, tipos num√©ricos e textos).\n",
    "3.  Armazena em um banco de dados local otimizado (DuckDB).\n",
    "4.  Gera visualiza√ß√µes geogr√°ficas (mapas de calor) e temporais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0abf23",
   "metadata": {},
   "source": [
    "### Dicion√°rio de Dados ‚Äî Mapeamento e Tipos\n",
    "\n",
    "A tabela abaixo descreve como as colunas originais do arquivo CSV/ZIP foram renomeadas e tipadas para an√°lise neste notebook.\n",
    "\n",
    "| **Coluna Original** | **Nome Final (Normalizado)** | **Tipo SQL** |\n",
    "|-------------------|------------------------------|--------------|\n",
    "| `regiao` | `regiao_brasil_hospital` | `VARCHAR(12)` |\n",
    "| `uf` | `uf_hospital` | `CHAR(2)` |\n",
    "| `co_ibge` | `codigo_ibge` | `VARCHAR(7)` |\n",
    "| `municipio` | `municipio_hospital` | `VARCHAR(60)` |\n",
    "| `cnes` | `cnes` | `VARCHAR(7)` |\n",
    "| `nome_estabelecimento` | `nome_hospital` | `VARCHAR(200)` |\n",
    "| `tp_gestao` | `tipo_gestao_do_hospital` | `CHAR(1)` |\n",
    "| `natureza_juridica` | `natureza_juridica_do_hospital` | `VARCHAR(4)` |\n",
    "| `leitos_sus` | `leitos_sus` | `INT` |\n",
    "| `leitos_existentes` | `leitos_geral` | `INT` |\n",
    "| `uti_total_existente` | `uti_total` | `INT` |\n",
    "| `uti_total_sus` | `uti_sus_total` | `INT` |\n",
    "| `comp` | `data_competencia_info` | `DATE` |\n",
    "*(...e demais colunas de UTI espec√≠ficas)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9721fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. DATA SOURCES ---\n",
    "DATASET_URLS = {\n",
    "    \"hospitais_leitos\": \"https://opendatasus.saude.gov.br/dataset/hospitais-e-leitos\"\n",
    "}\n",
    "\n",
    "# --- 2. FINAL TABLE SCHEMAS ---\n",
    "SCHEMA_MAPS = {\n",
    "    \"hospitais_leitos\": {\n",
    "        'regiao': {\n",
    "            'nome_final': 'regiao_brasil_hospital',\n",
    "            'tipo_sql': 'VARCHAR(12)'\n",
    "        },\n",
    "        'uf': {\n",
    "            'nome_final': 'uf_hospital',\n",
    "            'tipo_sql': 'CHAR(2)'\n",
    "        },\n",
    "        'co_ibge': {\n",
    "            'nome_final': 'codigo_ibge',\n",
    "            'tipo_sql': 'VARCHAR(7)'\n",
    "        },\n",
    "        'municipio': {\n",
    "            'nome_final': 'municipio_hospital',\n",
    "            'tipo_sql': 'VARCHAR(60)'\n",
    "        },\n",
    "        'cnes': {\n",
    "            'nome_final': 'cnes',\n",
    "            'tipo_sql': 'VARCHAR(7)'\n",
    "        },\n",
    "        'no_logradouro': {\n",
    "            'nome_final': 'endereco_hospital',\n",
    "            'tipo_sql': 'VARCHAR(60)'\n",
    "        },\n",
    "        'nu_endereco': {\n",
    "            'nome_final': 'numero_endereco_hospital',\n",
    "            'tipo_sql': 'VARCHAR(10)'\n",
    "        },\n",
    "        'no_complemento': {\n",
    "            'nome_final': 'complemento_endereco_hospital',\n",
    "            'tipo_sql': 'VARCHAR(20)'\n",
    "        },\n",
    "        'no_bairro': {\n",
    "            'nome_final': 'bairro_hospital',\n",
    "            'tipo_sql': 'VARCHAR(60)'\n",
    "        },\n",
    "        'co_cep': {\n",
    "            'nome_final': 'cep_hospital',\n",
    "            'tipo_sql': 'CHAR(8)'\n",
    "        },\n",
    "        'nome_estabelecimento': {\n",
    "            'nome_final': 'nome_hospital',\n",
    "            'tipo_sql': 'VARCHAR(200)'\n",
    "        },\n",
    "        'razao_social': {\n",
    "            'nome_final': 'nome_razao_social_hospital',\n",
    "            'tipo_sql': 'VARCHAR(60)'\n",
    "        },\n",
    "        'tp_gestao': {\n",
    "            'nome_final': 'tipo_gestao_do_hospital',\n",
    "            'tipo_sql': 'CHAR(1)'\n",
    "        },\n",
    "        'co_tipo_unidade': {\n",
    "            'nome_final': 'codigo_tipo_da_unidade',\n",
    "            'tipo_sql': 'VARCHAR(2)'\n",
    "        },\n",
    "        'ds_tipo_unidade': {\n",
    "            'nome_final': 'descricao_do_tipo_da_unidade',\n",
    "            'tipo_sql': 'VARCHAR(60)'\n",
    "        },\n",
    "        'natureza_juridica': {\n",
    "            'nome_final': 'natureza_juridica_do_hospital',\n",
    "            'tipo_sql': 'VARCHAR(4)'\n",
    "        },\n",
    "        'desc_natureza_juridica': {\n",
    "            'nome_final': 'descricao_da_natureza_juridica_do_hospital',\n",
    "            'tipo_sql': 'VARCHAR(60)'\n",
    "        },\n",
    "        'motivo_desabilitacao': {\n",
    "            'nome_final': 'motivo_desabilitacao_hospital',\n",
    "            'tipo_sql': 'VARCHAR(60)'\n",
    "        },\n",
    "        'no_email': {\n",
    "            'nome_final': 'email',\n",
    "            'tipo_sql': 'VARCHAR(60)'\n",
    "        },\n",
    "        'nu_telefone': {\n",
    "            'nome_final': 'telefone',\n",
    "            'tipo_sql': 'VARCHAR(40)'\n",
    "        },\n",
    "        'leitos_sus': {\n",
    "            'nome_final': 'leitos_sus',\n",
    "            'tipo_sql': 'INT'\n",
    "        },\n",
    "        'leitos_existentes': {\n",
    "            'nome_final': 'leitos_geral',\n",
    "            'tipo_sql': 'INT',\n",
    "            'aliases': ['leitos_existente']\n",
    "        },\n",
    "        'uti_total_existente': {\n",
    "            'nome_final': 'uti_total',\n",
    "            'tipo_sql': 'INT',\n",
    "            'aliases': ['uti_total___exist', 'uti_total_exist']\n",
    "        },\n",
    "        'uti_total_sus': {\n",
    "            'nome_final': 'uti_sus_total',\n",
    "            'tipo_sql': 'INT',\n",
    "            'aliases': ['uti_total___sus']\n",
    "        },\n",
    "        'uti_adulto_existente': {\n",
    "            'nome_final': 'uti_adulto',\n",
    "            'tipo_sql': 'INT',\n",
    "            'aliases': ['uti_adulto___exist', 'uti_adulto_exist']\n",
    "        },\n",
    "        'uti_adulto_sus': {\n",
    "            'nome_final': 'uti_sus_adulto',\n",
    "            'tipo_sql': 'INT',\n",
    "            'aliases': ['uti_adulto___sus']\n",
    "        },\n",
    "        'uti_pediatrico_existente': {\n",
    "            'nome_final': 'uti_pediatrico',\n",
    "            'tipo_sql': 'INT',\n",
    "            'aliases': ['uti_pediatrico___exist', 'uti_pediatrico_exist']\n",
    "        },\n",
    "        'uti_pediatrico_sus': {\n",
    "            'nome_final': 'uti_sus_pediatrico',\n",
    "            'tipo_sql': 'INT',\n",
    "            'aliases': ['uti_pediatrico___sus']\n",
    "        },\n",
    "        'uti_neonatal_existente': {\n",
    "            'nome_final': 'uti_neonatal',\n",
    "            'tipo_sql': 'INT',\n",
    "            'aliases': ['uti_neonatal___exist', 'uti_neonatal_exist']\n",
    "        },\n",
    "        'uti_neonatal_sus': {\n",
    "            'nome_final': 'uti_sus_neonatal',\n",
    "            'tipo_sql': 'INT',\n",
    "            'aliases': ['uti_neonatal___sus']\n",
    "        },\n",
    "        'uti_queimado_existente': {\n",
    "            'nome_final': 'uti_queimado',\n",
    "            'tipo_sql': 'INT',\n",
    "            'aliases': ['uti_queimado___exist', 'uti_queimado_exist']\n",
    "        },\n",
    "        'uti_queimado_sus': {\n",
    "            'nome_final': 'uti_sus_queimado',\n",
    "            'tipo_sql': 'INT',\n",
    "            'aliases': ['uti_queimado___sus']\n",
    "        },\n",
    "        'uti_coronariana_existente': {\n",
    "            'nome_final': 'uti_coronariana',\n",
    "            'tipo_sql': 'INT',\n",
    "            'aliases': ['uti_coronariana___exist', 'uti_coronariana_exist']\n",
    "        },\n",
    "        'uti_coronariana_sus': {\n",
    "            'nome_final': 'uti_sus_coronariana',\n",
    "            'tipo_sql': 'INT',\n",
    "            'aliases': ['uti_coronariana___sus']\n",
    "        },\n",
    "        'comp': {\n",
    "            'nome_final': 'data_competencia_info',\n",
    "            'tipo_sql': 'DATE',\n",
    "            'date_format': '%Y%m'\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fdc0da",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# TEMPO DE EXECU√á√ÉO ~5 minutos\n",
    "\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import warnings\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import zipfile\n",
    "import io\n",
    "import time\n",
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 1. CONFIGURA√á√ïES GERAIS E LOGGING ---\n",
    "# ==============================================================================\n",
    "\n",
    "DB_FILENAME = \"datasus.db\"\n",
    "BASE_URL = \"https://opendatasus.saude.gov.br\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "warnings.filterwarnings('ignore', category=pd.errors.ParserWarning)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- FUN√á√ÉO DE NORMALIZA√á√ÉO ---\n",
    "# ==============================================================================\n",
    "\n",
    "def normalize_name(name):\n",
    "    if not isinstance(name, str):\n",
    "        name = str(name)\n",
    "\n",
    "    nfkd_form = unicodedata.normalize('NFKD', name)\n",
    "    name_sem_acentos = \"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "    name_lower = name_sem_acentos.lower()\n",
    "\n",
    "    name_clean = re.sub(r'[^a-z0-9]+', '_', name_lower).strip('_')\n",
    "\n",
    "    return name_clean\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 2. FUN√á√ïES AUXILIARES DE REQUISI√á√ÉO E LEITURA ---\n",
    "# ==============================================================================\n",
    "def fetch_page_with_retries(url, max_retries=5, delay_seconds=10):\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            logging.info(f\"Acessando p√°gina: {url} (tentativa {attempt}/{max_retries})\")\n",
    "            response = requests.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return soup(response.content, \"html.parser\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.warning(f\"Tentativa {attempt} falhou: {e}\")\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(delay_seconds)\n",
    "    logging.error(f\"Falha ao acessar a p√°gina {url} ap√≥s {max_retries} tentativas.\")\n",
    "    return None\n",
    "\n",
    "def read_csv_with_detection(file_bytes_io):\n",
    "    possible_separators = [';', ',']\n",
    "    possible_encodings = ['utf-8-sig', 'latin-1', 'cp1252']\n",
    "\n",
    "    for enc in possible_encodings:\n",
    "        file_bytes_io.seek(0)\n",
    "        raw_text = file_bytes_io.read().decode(enc, errors='replace')\n",
    "\n",
    "        raw_text = raw_text.replace('\\x00', '')\n",
    "\n",
    "        sep_counts = {sep: raw_text.count(sep) for sep in possible_separators}\n",
    "        sep = max(sep_counts, key=sep_counts.get)\n",
    "\n",
    "        lines = raw_text.splitlines()\n",
    "\n",
    "        issue_detected = any(\n",
    "            line and not line.startswith('\"') and f'{sep}\"' in line\n",
    "            for line in lines[:10]\n",
    "        )\n",
    "\n",
    "        if issue_detected:\n",
    "            logging.warning(\"Detectado CSV malformado (primeiro campo sem aspas). Aplicando corre√ß√£o em todo o arquivo.\")\n",
    "            corrected_lines = []\n",
    "            for line in lines:\n",
    "                if line and not line.startswith('\"') and f'{sep}\"' in line:\n",
    "                    sep_index = line.find(sep)\n",
    "                    if sep_index != -1:\n",
    "                        first_field = line[:sep_index]\n",
    "                        corrected_line = f'\"{first_field}\"{line[sep_index:]}'\n",
    "                        corrected_lines.append(corrected_line)\n",
    "                    else:\n",
    "                        corrected_lines.append(line)\n",
    "                else:\n",
    "                    corrected_lines.append(line)\n",
    "\n",
    "            raw_text = \"\\n\".join(corrected_lines)\n",
    "        else:\n",
    "            raw_text = \"\\n\".join(lines)\n",
    "\n",
    "        cleaned_bytes = io.BytesIO(raw_text.encode('utf-8'))\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                cleaned_bytes,\n",
    "                sep=sep,\n",
    "                engine='python',\n",
    "                dtype=str,\n",
    "                on_bad_lines='skip',\n",
    "                quotechar='\"',\n",
    "            )\n",
    "\n",
    "            if df.shape[1] > 2 and df.columns.notna().all():\n",
    "                logging.info(f\"SUCESSO: encoding={enc}, separador='{sep}', colunas={len(df.columns)}\")\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Falha com encoding={enc}, sep='{sep}': {e}\")\n",
    "\n",
    "    logging.error(\"Nenhuma combina√ß√£o de encoding/separador funcionou para este arquivo.\")\n",
    "    return None\n",
    "\n",
    "def download_and_read_data_from_url(url):\n",
    "    logging.info(f\"Baixando dados de: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, timeout=180)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Falha ao baixar {url}: {e}\")\n",
    "        return None\n",
    "    file_bytes_io = io.BytesIO(response.content)\n",
    "    if url.lower().endswith('.zip'):\n",
    "        logging.info(\"Arquivo ZIP detectado. Descompactando em mem√≥ria...\")\n",
    "        try:\n",
    "            with zipfile.ZipFile(file_bytes_io) as zf:\n",
    "                csv_filenames = [f for f in zf.namelist() if f.lower().endswith('.csv')]\n",
    "                if not csv_filenames:\n",
    "                    logging.error(\"Nenhum .csv encontrado dentro do ZIP.\")\n",
    "                    return None\n",
    "                logging.info(f\"Extraindo: {csv_filenames[0]}\")\n",
    "                csv_bytes = zf.read(csv_filenames[0])\n",
    "                return read_csv_with_detection(io.BytesIO(csv_bytes))\n",
    "        except Exception as e:\n",
    "            logging.error(f\"ERRO ao processar o arquivo ZIP: {e}\")\n",
    "            return None\n",
    "    elif url.lower().endswith('.csv'):\n",
    "        logging.info(\"Arquivo CSV direto detectado.\")\n",
    "        return read_csv_with_detection(file_bytes_io)\n",
    "    else:\n",
    "        file_extension = url.split('.')[-1].upper()\n",
    "        logging.warning(f\"Formato de arquivo n√£o reconhecido ({file_extension}), pulando: {url}\")\n",
    "        return None\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 3. L√ìGICA PRINCIPAL DE ETL ---\n",
    "# ==============================================================================\n",
    "\n",
    "def main_etl_process():\n",
    "    logging.info(\"--- Iniciando ETL com recarga completa dos dados ---\")\n",
    "    with duckdb.connect(database=DB_FILENAME, read_only=False) as con:\n",
    "        for dataset_key, dataset_page_url in DATASET_URLS.items():\n",
    "            logging.info(f\"\\n{'='*60}\\nProcessando dataset: '{dataset_key}'\\n{'='*60}\")\n",
    "            schema_map = SCHEMA_MAPS.get(dataset_key)\n",
    "            if not schema_map:\n",
    "                logging.error(f\"Esquema n√£o encontrado para o dataset '{dataset_key}'. Pulando.\")\n",
    "                continue\n",
    "\n",
    "            table_name = normalize_name(dataset_key)\n",
    "            try:\n",
    "                sql_columns = [f'\"{normalize_name(prop[\"nome_final\"])}\" {prop[\"tipo_sql\"]}' for prop in schema_map.values()]\n",
    "                create_table_sql = f\"CREATE OR REPLACE TABLE {table_name} ({', '.join(sql_columns)});\"\n",
    "                con.execute(create_table_sql)\n",
    "                logging.info(f\"Tabela '{table_name}' criada/substitu√≠da com o esquema final.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"ERRO FATAL ao criar a tabela '{table_name}': {e}\")\n",
    "                continue\n",
    "\n",
    "            main_page_html = fetch_page_with_retries(dataset_page_url)\n",
    "            if not main_page_html: continue\n",
    "            resources = main_page_html.find_all(\"li\", class_=\"resource-item\")\n",
    "            if not resources:\n",
    "                logging.error(\"Nenhum recurso para download encontrado na p√°gina do dataset.\")\n",
    "                continue\n",
    "            logging.info(f\"Encontrados {len(resources)} arquivos/recursos potenciais para processar.\")\n",
    "            for resource in resources:\n",
    "                link_tag = resource.find(\"a\", class_=\"resource-url-analytics\")\n",
    "                if not link_tag or not link_tag.has_attr('href'): continue\n",
    "                href = link_tag['href']\n",
    "                final_download_url = None\n",
    "                if href.lower().endswith(('.csv', '.zip')):\n",
    "                    final_download_url = urljoin(BASE_URL, href)\n",
    "                elif href.lower().endswith(('.pdf', 'xml', 'json')):\n",
    "                    logging.info(f\"Recurso do tipo PDF encontrado e ignorado: {href}\")\n",
    "                    continue\n",
    "                else:\n",
    "                    resource_page_url = urljoin(BASE_URL, href)\n",
    "                    resource_page_html = fetch_page_with_retries(resource_page_url)\n",
    "                    if resource_page_html:\n",
    "                        download_button = resource_page_html.find(\"a\", class_=\"btn-primary\")\n",
    "                        if download_button and download_button.has_attr('href'):\n",
    "                            final_download_url = urljoin(BASE_URL, download_button['href'])\n",
    "                        else:\n",
    "                            logging.warning(f\"Link n√£o √© um arquivo direto e bot√£o de download n√£o foi encontrado em {resource_page_url}. Pulando.\")\n",
    "                if not final_download_url: continue\n",
    "                logging.info(f\"\\n--- Processando arquivo: {final_download_url.split('/')[-1]} ---\")\n",
    "                raw_df = download_and_read_data_from_url(final_download_url)\n",
    "                if raw_df is None or raw_df.empty:\n",
    "                    logging.warning(\"Arquivo pulado devido a falha no download ou leitura.\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    source_to_final_map = {}\n",
    "                    final_to_schema_map = {}\n",
    "                    for canonical_name, properties in schema_map.items():\n",
    "                        final_name = normalize_name(properties['nome_final'])\n",
    "                        if final_name not in final_to_schema_map:\n",
    "                            final_to_schema_map[final_name] = properties\n",
    "                        source_to_final_map[normalize_name(canonical_name)] = final_name\n",
    "                        if 'aliases' in properties:\n",
    "                            for alias in properties['aliases']:\n",
    "                                source_to_final_map[normalize_name(alias)] = final_name\n",
    "\n",
    "                    rename_map = {}\n",
    "                    final_names_used = set()\n",
    "                    for source_col in raw_df.columns:\n",
    "                        norm_source_col = normalize_name(source_col)\n",
    "                        if norm_source_col in source_to_final_map:\n",
    "                            final_name = source_to_final_map[norm_source_col]\n",
    "                            if final_name not in final_names_used:\n",
    "                                rename_map[source_col] = final_name\n",
    "                                final_names_used.add(final_name)\n",
    "                            else:\n",
    "                                logging.warning(\n",
    "                                    f\"Mapeamento duplicado para a coluna final '{final_name}'. \"\n",
    "                                    f\"A coluna de origem '{source_col}' ser√° ignorada.\"\n",
    "                                )\n",
    "\n",
    "                    processing_df = raw_df[list(rename_map.keys())].rename(columns=rename_map)\n",
    "\n",
    "                    final_df = pd.DataFrame()\n",
    "                    for final_name, column_data in processing_df.items():\n",
    "                        properties = final_to_schema_map.get(final_name)\n",
    "                        if not properties:\n",
    "                            final_df[final_name] = column_data.astype(str)\n",
    "                            continue\n",
    "\n",
    "                        sql_type = properties['tipo_sql'].upper()\n",
    "                        clean_column = column_data.replace({'NULL': None, '': None})\n",
    "\n",
    "                        if 'CHAR' in sql_type:\n",
    "                            match = re.search(r'\\((\\d+)\\)', sql_type)\n",
    "                            length = int(match.group(1)) if match else None\n",
    "                            final_df[final_name] = clean_column.astype(str).str.slice(0, length)\n",
    "\n",
    "                        elif 'INT' in sql_type or 'BIGINT' in sql_type:\n",
    "                            numeric_series = pd.to_numeric(\n",
    "                                clean_column.astype(str).str.replace(',', '.', regex=False),\n",
    "                                errors='coerce'\n",
    "                            )\n",
    "                            potential_loss_mask = (numeric_series.notna()) & ((numeric_series.fillna(0) % 1) != 0)\n",
    "                            if potential_loss_mask.any():\n",
    "                                examples = numeric_series[potential_loss_mask].head(3).tolist()\n",
    "                                logging.warning(\n",
    "                                    f\"Perda de precis√£o ao for√ßar INT na coluna '{final_name}'. \"\n",
    "                                    f\"Valores decimais ser√£o truncados. Exemplos: {examples}\"\n",
    "                                )\n",
    "                            is_null_mask = numeric_series.isnull()\n",
    "                            series_as_int = numeric_series.fillna(0).astype(int)\n",
    "                            final_df[final_name] = series_as_int.astype('Int64').where(~is_null_mask, pd.NA)\n",
    "\n",
    "                        elif 'DATE' in sql_type:\n",
    "                            date_format = properties.get('date_format')\n",
    "                            converted_dates = pd.to_datetime(\n",
    "                                clean_column,\n",
    "                                format=date_format,\n",
    "                                errors='coerce'\n",
    "                            )\n",
    "\n",
    "                            if converted_dates.isnull().all() and date_format:\n",
    "                                logging.warning(f\"Formato de data '{date_format}' falhou para a coluna '{final_name}'. Tentando infer√™ncia autom√°tica.\")\n",
    "                                converted_dates = pd.to_datetime(clean_column, errors='coerce')\n",
    "\n",
    "                            final_df[final_name] = converted_dates.dt.date\n",
    "\n",
    "                        elif 'DECIMAL' in sql_type:\n",
    "                            final_df[final_name] = pd.to_numeric(clean_column.astype(str).str.replace(',', '.', regex=False), errors='coerce')\n",
    "                        else:\n",
    "                            final_df[final_name] = clean_column.astype(str)\n",
    "\n",
    "                    if not final_df.empty:\n",
    "                        view_name = \"novos_dados_temp\"\n",
    "                        con.register(view_name, final_df)\n",
    "                        insert_sql = f\"INSERT INTO {table_name} BY NAME SELECT * FROM {view_name};\"\n",
    "                        con.execute(insert_sql)\n",
    "                        logging.info(f\"-> Inseridas {len(final_df)} linhas na tabela '{table_name}'.\")\n",
    "                        con.unregister(view_name)\n",
    "                    else:\n",
    "                        logging.info(\"Nenhuma coluna mapeada encontrada no arquivo. Nada a inserir.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"ERRO GERAL ao transformar ou carregar dados de {final_download_url}: {e}\", exc_info=True)\n",
    "\n",
    "    logging.info(f\"\\n--- Processo finalizado! Dados em '{DB_FILENAME}' ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_etl_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6043b2c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "DB_FILENAME = 'datasus.db'\n",
    "TABLE_NAME = 'hospitais_leitos'\n",
    "\n",
    "try:\n",
    "    con = duckdb.connect(database=DB_FILENAME, read_only=True)\n",
    "\n",
    "    print(f\"--- Colunas encontradas na tabela '{TABLE_NAME}' ---\")\n",
    "\n",
    "    schema_info = con.execute(f'DESCRIBE \"{TABLE_NAME}\";').fetchdf()\n",
    "\n",
    "    if not schema_info.empty:\n",
    "        for col_name in schema_info['column_name']:\n",
    "            print(col_name)\n",
    "    else:\n",
    "        print(f\"N√£o foi poss√≠vel encontrar a tabela '{TABLE_NAME}'.\")\n",
    "\n",
    "except duckdb.Error as e:\n",
    "    print(f\"\\nERRO DuckDB: N√£o foi poss√≠vel ler a tabela.\")\n",
    "    print(f\"Verifique se o arquivo '{DB_FILENAME}' existe e se a tabela '{TABLE_NAME}' est√° correta.\")\n",
    "    print(f\"Detalhe do erro: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nERRO inesperado: {e}\")\n",
    "\n",
    "finally:\n",
    "    if 'con' in locals() and con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5dd74f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "DB_FILENAME = 'datasus.db'\n",
    "TABLE_NAME = 'hospitais_leitos'\n",
    "\n",
    "try:\n",
    "    con = duckdb.connect(database=DB_FILENAME, read_only=True)\n",
    "\n",
    "    sql_query = f\"\"\"\n",
    "    SELECT DISTINCT ON (cnes) *\n",
    "    FROM \"{TABLE_NAME}\"\n",
    "    ORDER BY cnes, data_competencia_info DESC;\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Executando a consulta SQL na tabela '{TABLE_NAME}'...\")\n",
    "\n",
    "    result_df = con.execute(sql_query).fetchdf()\n",
    "\n",
    "    output_csv_filename = 'hospitais_leitos_latest.csv'\n",
    "\n",
    "    result_df.to_csv(output_csv_filename, index=False)\n",
    "\n",
    "    print(f\"Dados exportados com sucesso para '{output_csv_filename}'.\")\n",
    "    print(f\"N√∫mero de linhas exportadas: {len(result_df)}\")\n",
    "\n",
    "except duckdb.Error as e:\n",
    "    print(f\"\\nERRO DuckDB ao executar a consulta ou exportar para CSV:\")\n",
    "    print(f\"Verifique se o arquivo '{DB_FILENAME}' existe e se a tabela '{TABLE_NAME}' est√° correta.\")\n",
    "    print(f\"Detalhe do erro: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nERRO inesperado: {e}\")\n",
    "\n",
    "finally:\n",
    "    if 'con' in locals() and con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b6194",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "DB_FILENAME = 'datasus.db'\n",
    "TABLE_NAME = 'hospitais_leitos'\n",
    "\n",
    "try:\n",
    "    con = duckdb.connect(database=DB_FILENAME, read_only=True)\n",
    "\n",
    "    # SQL query to select all records\n",
    "    sql_query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM \"{TABLE_NAME}\";\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Executando a consulta SQL para todos os registros na tabela '{TABLE_NAME}'...\")\n",
    "\n",
    "    result_df = con.execute(sql_query).fetchdf()\n",
    "\n",
    "    # Output filename for the complete data\n",
    "    output_csv_filename = 'hospitais_leitos_completo.csv'\n",
    "\n",
    "    result_df.to_csv(output_csv_filename, index=False)\n",
    "\n",
    "    print(f\"Dados exportados com sucesso para '{output_csv_filename}'.\")\n",
    "    print(f\"N√∫mero de linhas exportadas: {len(result_df)}\")\n",
    "\n",
    "except duckdb.Error as e:\n",
    "    print(f\"\\nERRO DuckDB ao executar a consulta ou exportar para CSV:\")\n",
    "    print(f\"Verifique se o arquivo '{DB_FILENAME}' existe e se a tabela '{TABLE_NAME}' est√° correta.\")\n",
    "    print(f\"Detalhe do erro: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nERRO inesperado: {e}\")\n",
    "\n",
    "finally:\n",
    "    if 'con' in locals() and con:\n",
    "        con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef16bee",
   "metadata": {},
   "source": [
    "## Visualiza√ß√£o 1: Mapas de Calor por Estado (UF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d8181",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Gerador de Heatmaps de Leitos por Estado (UF) no Brasil (Plotly)\n",
    "Este script gera m√∫ltiplos mapas (um para cada tipo de leito)\n",
    "baseado no script de UFs.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import plotly.io as pio # Import plotly.io\n",
    "\n",
    "def criar_mapa_para_coluna_uf(df_base, brazil_states_geojson, col_leitos, coluna_estado):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o auxiliar que gera um mapa de calor de UF para uma coluna de dados espec√≠fica.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- üó∫Ô∏è  Processando UF: {col_leitos} ---\")\n",
    "\n",
    "    # 1. Copiar e processar a coluna de dados\n",
    "    df = df_base.copy()\n",
    "\n",
    "    df[col_leitos] = pd.to_numeric(df[col_leitos], errors='coerce').fillna(0)\n",
    "\n",
    "    # 2. Agregar dados\n",
    "    dados_agregados = df.groupby(coluna_estado)[col_leitos].sum().reset_index()\n",
    "\n",
    "    # Renomear para o plot\n",
    "    dados_agregados = dados_agregados.rename(columns={col_leitos: \"valor\"})\n",
    "\n",
    "    # Filtrar UFs sem dados\n",
    "    dados_agregados = dados_agregados[dados_agregados[\"valor\"] > 0]\n",
    "\n",
    "    if dados_agregados.empty:\n",
    "        print(f\"‚ÑπÔ∏è  Sem dados v√°lidos (> 0) para '{col_leitos}'. Mapa n√£o ser√° gerado.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üìä Total de UFs com dados para '{col_leitos}': {len(dados_agregados)}\")\n",
    "\n",
    "    # 3. Preparar Nomes\n",
    "    titulo_grafico = col_leitos.replace('_', ' ').replace('uti', 'UTI').title()\n",
    "\n",
    "    # 4. Gerar Mapa\n",
    "    fig = px.choropleth(\n",
    "        dados_agregados,\n",
    "        geojson=brazil_states_geojson,\n",
    "        locations=coluna_estado,        # Coluna com as siglas (ex: \"MG\")\n",
    "        featureidkey=\"properties.sigla\",  # Chave no GeoJSON com a sigla\n",
    "        color=\"valor\",                  # Coluna com os valores num√©ricos\n",
    "        color_continuous_scale=\"YlOrRd\",\n",
    "        scope=\"south america\",\n",
    "        labels={\"valor\": titulo_grafico},\n",
    "        title=f\"Heatmap de {titulo_grafico} por Estado no Brasil\",\n",
    "        # Removed template argument due to error\n",
    "        # template=pio.templates[\"plotly_white\"] # Use pio.templates instead of px.templates\n",
    "    )\n",
    "\n",
    "    fig.update_geos(fitbounds=\"locations\", visible=False)\n",
    "    fig.update_layout(margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0})\n",
    "\n",
    "    # ALTERA√á√ÉO 2: Substituir fig.write_html() por fig.show()\n",
    "    # 5. Exibir Resultado\n",
    "    fig.show()\n",
    "    print(f\"‚úÖ Mapa para '{col_leitos}' exibido acima.\")\n",
    "\n",
    "\n",
    "def gerar_todos_heatmaps_uf(csv_filename='hospitais_leitos_latest.csv'):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o principal que carrega os dados uma vez e depois gera\n",
    "    um mapa de calor de UF para cada coluna de leitos.\n",
    "    \"\"\"\n",
    "\n",
    "    # ==========================================================\n",
    "    # 1. LISTA DE COLUNAS PARA PROCESSAR\n",
    "    # ==========================================================\n",
    "    colunas_leitos_lista = [\n",
    "        'leitos_sus',\n",
    "        'leitos_geral',\n",
    "        'uti_total',\n",
    "        'uti_sus_total',\n",
    "        'uti_adulto',\n",
    "        'uti_sus_adulto',\n",
    "        'uti_pediatrico',\n",
    "        'uti_sus_pediatrico',\n",
    "        'uti_neonatal',\n",
    "        'uti_sus_neonatal',\n",
    "        'uti_queimado',\n",
    "        'uti_sus_queimado',\n",
    "        'uti_coronariana',\n",
    "        'uti_sus_coronariana'\n",
    "    ]\n",
    "\n",
    "    # Coluna de Estado (UF) do seu CSV\n",
    "    coluna_estado = 'uf_hospital'\n",
    "\n",
    "    # ==========================================================\n",
    "    # 2. BAIXAR GEOJSON (APENAS UMA VEZ)\n",
    "    # ==========================================================\n",
    "    geojson_url = \"https://raw.githubusercontent.com/codeforamerica/click_that_hood/master/public/data/brazil-states.geojson\"\n",
    "\n",
    "    print(\"Baixando o arquivo GeoJSON dos estados do Brasil...\")\n",
    "    try:\n",
    "        response = requests.get(geojson_url)\n",
    "        response.raise_for_status()\n",
    "        brazil_states_geojson = response.json()\n",
    "        print(\"‚úÖ Download do GeoJSON conclu√≠do com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao baixar o GeoJSON: {e}\")\n",
    "        return\n",
    "\n",
    "    # ==========================================================\n",
    "    # 3. CARREGAR CSV\n",
    "    # ==========================================================\n",
    "    try:\n",
    "        df = pd.read_csv(csv_filename, encoding='utf-8')\n",
    "        print(f\"‚úÖ Arquivo '{csv_filename}' carregado com sucesso.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Arquivo '{csv_filename}' n√£o encontrado.\")\n",
    "        return\n",
    "\n",
    "    # Checar se a coluna de UF existe\n",
    "    if coluna_estado not in df.columns:\n",
    "        print(f\"‚ùå Coluna de estado '{coluna_estado}' n√£o encontrada no CSV.\")\n",
    "        print(\"Colunas dispon√≠veis:\", list(df.columns))\n",
    "        return\n",
    "\n",
    "    # ==========================================================\n",
    "    # 4. PR√â-PROCESSAMENTO\n",
    "    # ==========================================================\n",
    "\n",
    "    # Garantir que as siglas da UF estejam limpas e mai√∫sculas\n",
    "    df[coluna_estado] = df[coluna_estado].astype(str).str.strip().str.upper()\n",
    "\n",
    "    # Corrigir siglas no GeoJSON\n",
    "    print(\"Processando GeoJSON para adicionar siglas...\")\n",
    "    for feature in brazil_states_geojson[\"features\"]:\n",
    "        props = feature[\"properties\"]\n",
    "        name = props.get(\"name\", \"\").upper()\n",
    "        # Mapeamento manual para garantir correspond√™ncia\n",
    "        nome_para_sigla = {\n",
    "            \"ACRE\": \"AC\", \"ALAGOAS\": \"AL\", \"AMAP√Å\": \"AP\", \"AMAPA\": \"AP\",\n",
    "            \"AMAZONAS\": \"AM\", \"BAHIA\": \"BA\", \"CEAR√Å\": \"CE\", \"CEARA\": \"CE\",\n",
    "            \"DISTRITO FEDERAL\": \"DF\", \"ESP√çRITO SANTO\": \"ES\", \"ESPIRITO SANTO\": \"ES\",\n",
    "            \"GOI√ÅS\": \"GO\", \"GOIAS\": \"GO\", \"MARANH√ÉO\": \"MA\", \"MARANHAO\": \"MA\",\n",
    "            \"MATO GROSSO\": \"MT\", \"MATO GROSSO DO SUL\": \"MS\", \"MINAS GERAIS\": \"MG\",\n",
    "            \"PAR√Å\": \"PA\", \"PARA\": \"PA\", \"PARA√çBA\": \"PB\", \"PARAIBA\": \"PB\",\n",
    "            \"PARAN√Å\": \"PR\", \"PARANA\": \"PR\", \"PERNAMBUCO\": \"PE\", \"PIAU√ç\": \"PI\",\n",
    "            \"PIAUI\": \"PI\", \"RIO DE JANEIRO\": \"RJ\", \"RIO GRANDE DO NORTE\": \"RN\",\n",
    "            \"RIO GRANDE DO SUL\": \"RS\", \"ROND√îNIA\": \"RO\", \"RONDONIA\": \"RO\",\n",
    "            \"RORAIMA\": \"RR\", \"SANTA CATARINA\": \"SC\", \"S√ÉO PAULO\": \"SP\", \"SAO PAULO\": \"SP\",\n",
    "            \"SERGIPE\": \"SE\", \"TOCANTINS\": \"TO\"\n",
    "        }\n",
    "        props[\"sigla\"] = nome_para_sigla.get(name, None)\n",
    "\n",
    "    # ==========================================================\n",
    "    # 5. LOOP DE GERA√á√ÉO DOS MAPAS\n",
    "    # ==========================================================\n",
    "    print(\"\\nIniciando gera√ß√£o dos mapas de UF em lote...\")\n",
    "\n",
    "    colunas_encontradas = 0\n",
    "    for coluna in colunas_leitos_lista:\n",
    "        if coluna not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è  Coluna '{coluna}' n√£o encontrada no CSV. Pulando...\")\n",
    "            continue\n",
    "\n",
    "        criar_mapa_para_coluna_uf(\n",
    "            df,\n",
    "            brazil_states_geojson,\n",
    "            coluna,\n",
    "            coluna_estado\n",
    "        )\n",
    "        colunas_encontradas += 1\n",
    "\n",
    "    print(f\"\\nüéâ Processo conclu√≠do! {colunas_encontradas} mapas de UF foram exibidos.\")\n",
    "\n",
    "# ==========================================================\n",
    "# Execu√ß√£o direta\n",
    "# ==========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    gerar_todos_heatmaps_uf(csv_filename='hospitais_leitos_latest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7c90d2",
   "metadata": {},
   "source": [
    "## Visualiza√ß√£o 2: Mapas de Calor por Munic√≠pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8848aca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import requests\n",
    "from unidecode import unidecode\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def create_municipality_heatmap_go(df_base, municipios_geojson, col_leitos, col_municipio):\n",
    "    \"\"\"\n",
    "    Generates a municipality-level heatmap using plotly.graph_objects.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- üó∫Ô∏è  Processing column (go.Choropleth): {col_leitos} ---\")\n",
    "\n",
    "    df = df_base.copy()\n",
    "    df[col_leitos] = pd.to_numeric(df[col_leitos], errors='coerce').fillna(0)\n",
    "\n",
    "    # Aggregate data\n",
    "    dados_agregados = (\n",
    "        df.groupby([\"MUN_KEY\", col_municipio], as_index=False)[col_leitos]\n",
    "        .sum()\n",
    "        .rename(columns={col_leitos: \"valor\"})\n",
    "    )\n",
    "\n",
    "    # Filter municipalities with no data (optional, depending on desired visualization)\n",
    "    # dados_agregados = dados_agregados[dados_agregados[\"valor\"] > 0]\n",
    "\n",
    "    if dados_agregados.empty or dados_agregados[\"valor\"].sum() == 0:\n",
    "        print(f\"‚ÑπÔ∏è  No valid data (> 0) found for '{col_leitos}'. Map will not be generated.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üìä Total municipalities with data for '{col_leitos}': {len(dados_agregados)}\")\n",
    "\n",
    "    # Prepare Names\n",
    "    titulo_grafico = col_leitos.replace('_', ' ').replace('uti', 'UTI').title()\n",
    "\n",
    "    # Create the figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add Choropleth trace\n",
    "    fig.add_trace(go.Choropleth(\n",
    "        geojson=municipios_geojson,\n",
    "        locations=dados_agregados[\"MUN_KEY\"],\n",
    "        z=dados_agregados[\"valor\"],\n",
    "        featureidkey=\"properties.NAME_KEY\",\n",
    "        colorscale=\"YlOrRd\",\n",
    "        colorbar_title=titulo_grafico.replace(\" \", \"<br>\"),\n",
    "        hovertext=dados_agregados[col_municipio], # Use original municipality name for hover\n",
    "        hoverinfo='text+z', # Show municipality name and value on hover\n",
    "        marker_line_width=0.5,\n",
    "        marker_opacity=0.8,\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Heatmap of {titulo_grafico} by Municipality in Brazil\",\n",
    "        geo_scope=\"south america\", # Focus on South America\n",
    "        geo=dict(\n",
    "            showcoastlines=False,\n",
    "            showland=True,\n",
    "            landcolor=\"white\",\n",
    "            showcountries=False,\n",
    "            showsubunits=False,\n",
    "            showframe=False,\n",
    "            fitbounds=\"locations\", # Adjust map bounds to fit the data locations\n",
    "        ),\n",
    "        margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0},\n",
    "        paper_bgcolor=\"white\",\n",
    "        plot_bgcolor=\"white\",\n",
    "    )\n",
    "\n",
    "    # Display the figure\n",
    "    fig.show()\n",
    "    print(f\"‚úÖ Map for '{col_leitos}' displayed above.\")\n",
    "\n",
    "\n",
    "def generate_all_municipality_heatmaps_go(csv_filename='hospitais_leitos_latest.csv'):\n",
    "    \"\"\"\n",
    "    Main function to load data and generate municipality heatmaps using go.Choropleth.\n",
    "    \"\"\"\n",
    "\n",
    "    # ==========================================================\n",
    "    # 1. LIST OF COLUMNS TO PROCESS\n",
    "    # ==========================================================\n",
    "    columns_leitos_list = [\n",
    "        'leitos_sus',\n",
    "        'leitos_geral',\n",
    "        'uti_total',\n",
    "        'uti_sus_total',\n",
    "        'uti_adulto',\n",
    "        'uti_sus_adulto',\n",
    "        'uti_pediatrico',\n",
    "        'uti_sus_pediatrico',\n",
    "        'uti_neonatal',\n",
    "        'uti_sus_neonatal',\n",
    "        'uti_queimado',\n",
    "        'uti_sus_queimado',\n",
    "        'uti_coronariana',\n",
    "        'uti_sus_coronariana'\n",
    "    ]\n",
    "\n",
    "    col_municipio = 'municipio_hospital'\n",
    "\n",
    "    # ==========================================================\n",
    "    # 2. DOWNLOAD GEOJSON (only once)\n",
    "    # ==========================================================\n",
    "    geojson_url = \"https://raw.githubusercontent.com/tbrugz/geodata-br/master/geojson/geojs-100-mun.json\"\n",
    "    print(\"Downloading simplified GeoJSON of Brazilian municipalities (~5MB)...\")\n",
    "    try:\n",
    "        response = requests.get(geojson_url)\n",
    "        response.raise_for_status()\n",
    "        municipios_geojson = response.json()\n",
    "        print(\"‚úÖ GeoJSON download successful.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading GeoJSON: {e}\")\n",
    "        return\n",
    "\n",
    "    # ==========================================================\n",
    "    # 3. LOAD CSV\n",
    "    # ==========================================================\n",
    "    try:\n",
    "        df = pd.read_csv(csv_filename, encoding='utf-8')\n",
    "        print(f\"‚úÖ File '{csv_filename}' loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File '{csv_filename}' not found.\")\n",
    "        return\n",
    "\n",
    "    # Check if essential columns exist\n",
    "    if col_municipio not in df.columns:\n",
    "        print(f\"‚ùå Municipality column '{col_municipio}' not found.\")\n",
    "        print(\"Please check if the column name in the CSV is correct.\")\n",
    "        print(\"Available columns:\", list(df.columns))\n",
    "        return\n",
    "\n",
    "    # ==========================================================\n",
    "    # 4. CLEANING AND NORMALIZATION\n",
    "    # ==========================================================\n",
    "    print(\"Normalizing municipality names...\")\n",
    "    df[col_municipio] = df[col_municipio].astype(str).str.strip().replace([\"\", \"nan\", \"None\"], np.nan)\n",
    "    df.dropna(subset=[col_municipio], inplace=True) # Drop rows where municipality is null\n",
    "\n",
    "    # Apply unidecode and uppercase for key creation\n",
    "    df[\"MUN_KEY\"] = df[col_municipio].apply(lambda x: unidecode(x).upper())\n",
    "\n",
    "    # Manual replacements for known mismatches\n",
    "    substituicoes = {\n",
    "        \"CEILANDIA\": \"BRASILIA\", \"SAMAMBAIA\": \"BRASILIA\", \"LAGO NORTE\": \"BRASILIA\",\n",
    "        \"LAGO SUL\": \"BRASILIA\", \"NUCLEO BANDEIRANTE\": \"BRASILIA\",\n",
    "        \"AUGUSTO SEVERO\": \"CAMPO GRANDE\", \"JANUARIO CICCO\": \"BOA SAUDE\",\n",
    "        \"MOJI MIRIM\": \"MOGI MIRIM\", \"PARATI\": \"PARATY\", \"POXOREO\": \"POXOREU\",\n",
    "        \"BRASOPOLIS\": \"BRAZOPOLIS\", \"IGUARACI\": \"IGUARACY\",\n",
    "        \"BELEM DE SAO FRANCISCO\": \"BELEM DO SAO FRANCISCO\",\n",
    "        \"LAGOA DO ITAENGA\": \"LAGOA DE ITAENGA\",\n",
    "        \"SAO LUIS DO PARAITINGA\": \"SAO LUIZ DO PARAITINGA\",\n",
    "        \"SAO VALERIO DA NATIVIDADE\": \"SAO VALERIO\", \"SERIDO\": \"CARNAUBA DOS DANTAS\",\n",
    "        \"TRAJANO DE MORAIS\": \"TRAJANO DE MORAES\"\n",
    "    }\n",
    "    df[\"MUN_KEY\"] = df[\"MUN_KEY\"].replace(substituicoes)\n",
    "\n",
    "    # Prepare GeoJSON keys\n",
    "    for feat in municipios_geojson[\"features\"]:\n",
    "        nome = feat[\"properties\"].get(\"name\", \"\")\n",
    "        feat[\"properties\"][\"NAME_KEY\"] = unidecode(nome).upper().strip()\n",
    "\n",
    "    # Filter out municipalities from df whose MUN_KEY is not in the GeoJSON keys\n",
    "    geojson_keys = {feat[\"properties\"][\"NAME_KEY\"] for feat in municipios_geojson[\"features\"]}\n",
    "    df = df[df[\"MUN_KEY\"].isin(geojson_keys)].copy()\n",
    "    print(f\"Filtered data: {len(df)} rows after matching with GeoJSON.\")\n",
    "\n",
    "\n",
    "    # ==========================================================\n",
    "    # 5. LOOP FOR GENERATING MAPS\n",
    "    # ==========================================================\n",
    "    print(\"\\nStarting batch map generation (using go.Choropleth)...\")\n",
    "\n",
    "    columns_found = 0\n",
    "    for column in columns_leitos_list:\n",
    "        if column not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è  Column '{column}' not found in CSV. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        create_municipality_heatmap_go(df, municipios_geojson, column, col_municipio)\n",
    "        columns_found += 1\n",
    "\n",
    "    print(f\"\\nüéâ Process complete! {columns_found} municipality maps attempted.\")\n",
    "\n",
    "# ==========================================================\n",
    "# Direct execution\n",
    "# ==========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Using the latest CSV file generated previously\n",
    "    generate_all_municipality_heatmaps_go(csv_filename='hospitais_leitos_latest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32a6abb",
   "metadata": {},
   "source": [
    "## Visualiza√ß√£o 3: Evolu√ß√£o Temporal das Categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d329579c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "import plotly.express as px\n",
    "import os\n",
    "import shutil # Usado para criar/apagar a pasta temp\n",
    "import re # Usaremos uma busca simples por <body>\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURA√á√ïES\n",
    "# ==============================================================================\n",
    "DB_FILENAME = 'datasus.db'\n",
    "TABLE_NAME = 'hospitais_leitos'\n",
    "\n",
    "# Diret√≥rio para salvar os arquivos tempor√°rios\n",
    "TEMP_DIR = \"temp_graficos_v20\"\n",
    "\n",
    "# Colunas de Categoria\n",
    "CATEGORIAS_PRINCIPAIS = [\n",
    "    'tipo_gestao_do_hospital',\n",
    "    'descricao_do_tipo_da_unidade',\n",
    "    'descricao_da_natureza_juridica_do_hospital'\n",
    "]\n",
    "\n",
    "# Colunas de M√©trica\n",
    "METRICAS_ANALISE = [\n",
    "    'leitos_sus',\n",
    "    'leitos_geral',\n",
    "    'uti_total',\n",
    "    'uti_sus_total',\n",
    "    'uti_adulto',\n",
    "    'uti_sus_adulto',\n",
    "    'uti_pediatrico',\n",
    "    'uti_sus_pediatrico',\n",
    "    'uti_neonatal',\n",
    "    'uti_sus_neonatal',\n",
    "    'uti_queimado',\n",
    "    'uti_sus_queimado',\n",
    "    'uti_coronariana',\n",
    "    'uti_sus_coronariana'\n",
    "]\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. FUN√á√ÉO AUXILIAR PARA EXTRAIR O CONTE√öDO DO <body>\n",
    "# ==============================================================================\n",
    "\n",
    "def extract_body_content(html_content):\n",
    "    \"\"\"\n",
    "    Extrai todo o conte√∫do entre as tags <body> e </body>.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Encontra o in√≠cio do conte√∫do (logo ap√≥s <body ...>)\n",
    "        body_start_match = re.search(r'<body.*?>', html_content, re.IGNORECASE | re.DOTALL)\n",
    "        if not body_start_match:\n",
    "            print(\"AVISO: Tag <body> n√£o encontrada no arquivo tempor√°rio.\")\n",
    "            return \"\"\n",
    "\n",
    "        start_index = body_start_match.end()\n",
    "\n",
    "        # Encontra o fim do conte√∫do (logo antes de </body>)\n",
    "        body_end_match = re.search(r'</body\\s*>', html_content, re.IGNORECASE | re.DOTALL)\n",
    "        if not body_end_match:\n",
    "            print(\"AVISO: Tag </body> n√£o encontrada no arquivo tempor√°rio.\")\n",
    "            return \"\"\n",
    "\n",
    "        end_index = body_end_match.start()\n",
    "\n",
    "        return html_content[start_index:end_index]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao extrair conte√∫do do body: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. PROCESSAMENTO E GERA√á√ÉO DE GR√ÅFICOS\n",
    "# ==============================================================================\n",
    "\n",
    "print(f\"Iniciando gera√ß√£o (v20 - Agrega√ß√£o Robusta)...\")\n",
    "\n",
    "# Limpa e cria o diret√≥rio tempor√°rio\n",
    "if os.path.exists(TEMP_DIR):\n",
    "    shutil.rmtree(TEMP_DIR)\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "print(f\"Diret√≥rio tempor√°rio criado em: {os.path.abspath(TEMP_DIR)}\")\n",
    "\n",
    "try:\n",
    "    con = duckdb.connect(database=DB_FILENAME, read_only=True)\n",
    "\n",
    "    # === LOOP EXTERNO (pelas 3 categorias) ===\n",
    "    for cat_col in CATEGORIAS_PRINCIPAIS:\n",
    "        print(f\"\\n========================================================\")\n",
    "        print(f\"Processando categoria: {cat_col}\")\n",
    "        print(f\"========================================================\")\n",
    "\n",
    "        temp_files_list = [] # Lista para guardar os NOMES dos arquivos tempor√°rios\n",
    "\n",
    "        # --- PARTE 1: Gerar os 14 arquivos HTML separados (L√≥gica v17) ---\n",
    "        for met_col in METRICAS_ANALISE:\n",
    "            print(f\"  -> Gerando arquivo tempor√°rio para: {met_col}\")\n",
    "            try:\n",
    "                # 1. Query DENTRO DO LOOP\n",
    "                # Certifica-se que a coluna de data √© CAST para VARCHAR ANTES de agrupar para evitar erros de tipo\n",
    "                query = f\"\"\"\n",
    "                SELECT\n",
    "                    CAST(\"data_competencia_info\" AS VARCHAR) AS ano_mes,\n",
    "                    \"{cat_col}\",\n",
    "                    SUM(CAST(\"{met_col}\" AS BIGINT)) AS \"{met_col}\"\n",
    "                FROM \"{TABLE_NAME}\"\n",
    "                WHERE\n",
    "                    \"data_competencia_info\" IS NOT NULL\n",
    "                    AND \"{cat_col}\" IS NOT NULL\n",
    "                GROUP BY ano_mes, \"{cat_col}\"\n",
    "                ORDER BY ano_mes, \"{cat_col}\"\n",
    "                \"\"\"\n",
    "                df_agregado = con.execute(query).fetchdf()\n",
    "\n",
    "                if df_agregado.empty:\n",
    "                    print(f\"    AVISO: Query vazia para '{met_col}'. Pulando.\")\n",
    "                    continue\n",
    "\n",
    "                # Garante que 'ano_mes' √© string antes de usar para pivot\n",
    "                df_agregado['ano_mes'] = df_agregado['ano_mes'].astype(str)\n",
    "                df_agregado = df_agregado.dropna(subset=['ano_mes', cat_col])\n",
    "\n",
    "                # Handle potential non-string categories before unique()\n",
    "                df_agregado[cat_col] = df_agregado[cat_col].astype(str)\n",
    "\n",
    "                todos_os_meses = sorted(df_agregado['ano_mes'].unique())\n",
    "\n",
    "                # Ensure that the pivot table uses string types for columns\n",
    "                df_pivot = df_agregado.pivot(index='ano_mes', columns=cat_col, values=met_col)\n",
    "                df_pivot = df_pivot.reindex(todos_os_meses)\n",
    "\n",
    "                # Convert index to datetime after reindexing\n",
    "                df_pivot.index = pd.to_datetime(df_pivot.index, format='%Y-%m-%d', errors='coerce') # Ajustado formato para YYYY-MM-DD\n",
    "                # Corrigido: Filtrar linhas onde a convers√£o da data falhou (√≠ndice √© NaT)\n",
    "                df_pivot = df_pivot[df_pivot.index.notna()]\n",
    "                df_pivot.index.name = 'Data'\n",
    "\n",
    "\n",
    "                # 5. Plotagem\n",
    "                title = f\"Evolu√ß√£o Mensal de {met_col.replace('_', ' ').title()} por {cat_col.replace('_', ' ').title()}\"\n",
    "                fig = px.line(df_pivot, title=title, markers=True, labels={'value': 'Total', cat_col: f'Categoria ({cat_col})'})\n",
    "                fig.update_traces(hovertemplate='<b>%{x|%Y-%m}</b><br>%{full_name}<br>Total: %{y}')\n",
    "\n",
    "                # 6. Salva o arquivo HTML COMPLETO\n",
    "                temp_filename = os.path.join(TEMP_DIR, f\"temp_grafico_{cat_col}_{met_col}.html\")\n",
    "                fig.write_html(temp_filename, include_plotlyjs='cdn') # Usa CDN para ser mais leve\n",
    "                temp_files_list.append(temp_filename)\n",
    "\n",
    "            except Exception as e_plot:\n",
    "                print(f\"    ERRO ao plotar '{met_col}': {e_plot}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "\n",
    "\n",
    "        # --- PARTE 2: O \"Agregador\" (L√≥gica Robusta) ---\n",
    "        if temp_files_list:\n",
    "            final_filename = f\"evolucao_por_{cat_col}_v20_FINAL.html\"\n",
    "            print(f\"\\n  -> Agregando {len(temp_files_list)} arquivos em '{final_filename}'...\")\n",
    "\n",
    "            with open(final_filename, 'w', encoding='utf-8') as f_out:\n",
    "                # Escreve o cabe√ßalho 1x\n",
    "                f_out.write(f\"\"\"<html>\n",
    "                <head>\n",
    "                    <title>Evolu√ß√£o por {cat_col.replace('_', ' ').title()}</title>\n",
    "                    <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "                </head>\n",
    "                <body style=\"font-family: sans-serif; padding: 20px; background-color: #f4f4f4;\">\n",
    "                    <h1 style=\"text-align: center;\">Relat√≥rio de Evolu√ß√£o por {cat_col.replace('_', ' ').title()}</h1>\n",
    "                \"\"\")\n",
    "\n",
    "                # Loop para ler cada arquivo tempor√°rio\n",
    "                for i, temp_file in enumerate(temp_files_list):\n",
    "                    if i > 0:\n",
    "                        f_out.write('<hr style=\"margin-top: 40px; margin-bottom: 40px;\">')\n",
    "\n",
    "                    try:\n",
    "                        with open(temp_file, 'r', encoding='utf-8') as f_in:\n",
    "                            content = f_in.read()\n",
    "\n",
    "                            # Usa a fun√ß√£o auxiliar para extrair o conte√∫do do body\n",
    "                            body_content = extract_body_content(content)\n",
    "\n",
    "                            if body_content:\n",
    "                                f_out.write(body_content) # Cola o conte√∫do do body\n",
    "                            else:\n",
    "                                f_out.write(f\"<p>ERRO: N√£o foi poss√≠vel parsear o gr√°fico de {temp_file}</p>\")\n",
    "                    except Exception as e_read:\n",
    "                         f_out.write(f\"<p>ERRO ao ler o arquivo tempor√°rio {temp_file}: {e_read}</p>\")\n",
    "\n",
    "\n",
    "                # Escreve o rodap√© 1x\n",
    "                f_out.write(\"</body></html>\")\n",
    "\n",
    "            print(f\"  -> ‚úÖ Arquivo final salvo com sucesso em '{os.path.abspath(final_filename)}'\")\n",
    "        else:\n",
    "            print(f\"  -> Nenhum gr√°fico foi gerado para '{cat_col}'.\")\n",
    "\n",
    "except duckdb.Error as e:\n",
    "    print(f\"\\nERRO DuckDB: {e}\")\n",
    "except KeyError as e:\n",
    "    print(f\"\\nERRO KeyError: A coluna {e} n√£o foi encontrada no DataFrame.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nERRO inesperado: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "finally:\n",
    "    if 'con' in locals() and con:\n",
    "        con.close()\n",
    "        print(\"\\nConex√£o DuckDB fechada.\")\n",
    "\n",
    "# Limpa a pasta tempor√°ria\n",
    "try:\n",
    "    if os.path.exists(TEMP_DIR):\n",
    "        shutil.rmtree(TEMP_DIR)\n",
    "        print(f\"\\nDiret√≥rio tempor√°rio '{TEMP_DIR}' removido.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAVISO: N√£o foi poss√≠vel remover o diret√≥rio tempor√°rio '{TEMP_DIR}': {e}\")\n",
    "\n",
    "print(\"\\nProcessamento conclu√≠do.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a3079c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "\n",
    "def analise_evolucao_temporal(caminho_csv):\n",
    "    print(f\"üìÇ Lendo arquivo: {caminho_csv}\")\n",
    "    df = pd.read_csv(caminho_csv)\n",
    "\n",
    "    # --- Padroniza nomes ---\n",
    "    colunas = [c.lower().strip() for c in df.columns]\n",
    "    mapa = dict(zip(colunas, df.columns))\n",
    "\n",
    "    if \"data_competencia_info\" not in colunas:\n",
    "        print(\"‚ùå Coluna 'data_competencia_info' n√£o encontrada.\")\n",
    "        print(\"‚úÖ Colunas dispon√≠veis:\", df.columns.tolist())\n",
    "        return\n",
    "\n",
    "    df.rename(columns={mapa[\"data_competencia_info\"]: \"data_competencia_info\"}, inplace=True)\n",
    "\n",
    "    # --- Convers√£o robusta das datas ---\n",
    "    print(\"üïê Convertendo coluna de datas...\")\n",
    "    df[\"data_competencia_info\"] = (\n",
    "        df[\"data_competencia_info\"].astype(str).str.strip().replace([\"\", \"nan\", \"None\"], np.nan)\n",
    "    )\n",
    "\n",
    "    df[\"data_competencia_info\"] = pd.to_datetime(df[\"data_competencia_info\"], errors=\"coerce\")\n",
    "\n",
    "    # Se mais da metade for NaT, tenta formato alternativo\n",
    "    if df[\"data_competencia_info\"].isna().mean() > 0.5:\n",
    "        df[\"data_competencia_info\"] = pd.to_datetime(\n",
    "            df[\"data_competencia_info\"], format=\"%Y%m\", errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "    print(\"Datas v√°lidas:\", df[\"data_competencia_info\"].notna().sum())\n",
    "    print(\"Datas inv√°lidas:\", df[\"data_competencia_info\"].isna().sum())\n",
    "\n",
    "    df.dropna(subset=[\"data_competencia_info\"], inplace=True)\n",
    "    if df.empty:\n",
    "        print(\"‚ùå Nenhuma linha v√°lida com data encontrada!\")\n",
    "        return\n",
    "\n",
    "    print(\"üìÖ Per√≠odo dos dados:\",\n",
    "          df[\"data_competencia_info\"].min().strftime(\"%Y-%m-%d\"),\n",
    "          \"‚Üí\",\n",
    "          df[\"data_competencia_info\"].max().strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "    # --- Detecta colunas de leitos ---\n",
    "    colunas_leitos = {\n",
    "        'leitos_geral': 'Leitos Gerais',\n",
    "        'leitos_sus': 'Leitos SUS',\n",
    "        'uti_total': 'UTI Total',\n",
    "        'uti_sus_total': 'UTI SUS Total'\n",
    "    }\n",
    "\n",
    "    colunas_encontradas = {}\n",
    "    for nome_col, titulo in colunas_leitos.items():\n",
    "        if nome_col in colunas:\n",
    "            colunas_encontradas[mapa[nome_col]] = titulo\n",
    "\n",
    "    if not colunas_encontradas:\n",
    "        print(\"‚ùå Nenhuma coluna de leitos encontrada!\")\n",
    "        print(\"‚úÖ Colunas dispon√≠veis:\", df.columns.tolist())\n",
    "        return\n",
    "\n",
    "    print(f\"üìä Colunas de leitos encontradas: {list(colunas_encontradas.keys())}\")\n",
    "\n",
    "    # Converte colunas para num√©rico\n",
    "    for col in colunas_encontradas.keys():\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # --- Verifica duplica√ß√µes por hospital e data ---\n",
    "    if \"cnes\" not in colunas:\n",
    "        print(\"‚ö†Ô∏è Coluna 'cnes' n√£o encontrada ‚Äì agregando diretamente por data.\")\n",
    "        df_agg = df.copy()\n",
    "    else:\n",
    "        col_cnes = mapa[\"cnes\"]\n",
    "        duplicadas = df.duplicated(subset=[col_cnes, \"data_competencia_info\"], keep=False)\n",
    "        n_dup = duplicadas.sum()\n",
    "        print(f\"üîç Registros duplicados por hospital/m√™s: {n_dup}\")\n",
    "\n",
    "        # Agrupa por hospital e compet√™ncia ‚Üí 1 linha por hospital/m√™s\n",
    "        agg_dict = {col: 'max' for col in colunas_encontradas.keys()}\n",
    "        df_agg = (\n",
    "            df.groupby([col_cnes, \"data_competencia_info\"])\n",
    "            .agg(agg_dict)\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "    # --- Soma total de leitos no Brasil por m√™s ---\n",
    "    agg_dict = {col: 'sum' for col in colunas_encontradas.keys()}\n",
    "    df_total = df_agg.groupby(\"data_competencia_info\").agg(agg_dict).reset_index()\n",
    "    df_total = df_total.sort_values(\"data_competencia_info\")\n",
    "\n",
    "    # --- Cria diret√≥rio de sa√≠da ---\n",
    "    os.makedirs(\"analises_leitos\", exist_ok=True)\n",
    "\n",
    "    # --- Gera UM √öNICO GR√ÅFICO com todas as curvas ---\n",
    "    print(f\"\\nüìà Gerando gr√°fico com {len(colunas_encontradas)} curvas...\")\n",
    "\n",
    "    # Cores para cada tipo de leito\n",
    "    cores = {\n",
    "        'Leitos Gerais': '#1f77b4',\n",
    "        'Leitos SUS': '#ff7f0e',\n",
    "        'UTI Total': '#2ca02c',\n",
    "        'UTI SUS Total': '#d62728'\n",
    "    }\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for col_original, titulo in colunas_encontradas.items():\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df_total[\"data_competencia_info\"],\n",
    "            y=df_total[col_original],\n",
    "            mode='lines+markers',\n",
    "            name=titulo,\n",
    "            line=dict(width=2, color=cores.get(titulo, '#333')),\n",
    "            marker=dict(size=4),\n",
    "            hovertemplate='<b>%{x|%Y-%m}</b><br>%{fullData.name}<br>Total: %{y:,.0f}<extra></extra>'\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Evolu√ß√£o Mensal de Leitos Hospitalares no Brasil',\n",
    "        xaxis_title='Data',\n",
    "        yaxis_title='Total',\n",
    "        hovermode='x unified',\n",
    "        template='plotly_white',\n",
    "        height=600,\n",
    "        legend=dict(\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=0.01,\n",
    "            bgcolor=\"rgba(255, 255, 255, 0.8)\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --- Exibe o gr√°fico em vez de salvar como HTML ---\n",
    "    fig.show()\n",
    "\n",
    "    print(\"\\nüéâ An√°lise temporal conclu√≠da com sucesso!\")\n",
    "\n",
    "\n",
    "# --- Execu√ß√£o direta ---\n",
    "if __name__ == \"__main__\":\n",
    "    analise_evolucao_temporal(\"hospitais_leitos_completo.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec67742",
   "metadata": {},
   "source": [
    "## Visualiza√ß√£o 4: Outras visualiza√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144d16c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURA√á√ïES\n",
    "# ==============================================================================\n",
    "\n",
    "OUTPUT_DIR = \"analises_leitos\"\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "# Dados de popula√ß√£o por UF (IBGE 2022 - estimativa)\n",
    "POPULACAO_UF = {\n",
    "    'SP': 44411238, 'MG': 20539989, 'RJ': 16055174, 'BA': 14141626,\n",
    "    'PR': 11444380, 'RS': 10882965, 'PE': 9058931, 'CE': 8794957,\n",
    "    'PA': 8120131, 'MA': 6776699, 'SC': 7610361, 'GO': 7056495,\n",
    "    'PB': 3974687, 'AM': 3941613, 'ES': 3833712, 'RN': 3302729,\n",
    "    'AL': 3127683, 'MT': 3658649, 'PI': 3271199, 'DF': 2817381,\n",
    "    'MS': 2757013, 'SE': 2210004, 'RO': 1581196, 'TO': 1511460,\n",
    "    'AC': 830018, 'AP': 733759, 'RR': 636707\n",
    "}\n",
    "\n",
    "# Regi√µes do Brasil\n",
    "REGIOES = {\n",
    "    'Norte': ['AC', 'AP', 'AM', 'PA', 'RO', 'RR', 'TO'],\n",
    "    'Nordeste': ['AL', 'BA', 'CE', 'MA', 'PB', 'PE', 'PI', 'RN', 'SE'],\n",
    "    'Centro-Oeste': ['DF', 'GO', 'MT', 'MS'],\n",
    "    'Sudeste': ['ES', 'MG', 'RJ', 'SP'],\n",
    "    'Sul': ['PR', 'RS', 'SC']\n",
    "}\n",
    "\n",
    "# Inverte o dicion√°rio para mapear UF -> Regi√£o\n",
    "UF_PARA_REGIAO = {}\n",
    "for regiao, ufs in REGIOES.items():\n",
    "    for uf in ufs:\n",
    "        UF_PARA_REGIAO[uf] = regiao\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. LEITOS PER CAPITA\n",
    "# ==============================================================================\n",
    "\n",
    "def analise_leitos_per_capita(csv_filename='last.csv'):\n",
    "    \"\"\"\n",
    "    Analisa leitos por 1.000 habitantes por estado.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üë• AN√ÅLISE 1: LEITOS PER CAPITA\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    print(f\"üìÇ Carregando arquivo '{csv_filename}'...\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_filename, encoding='utf-8')\n",
    "        print(f\"‚úÖ Carregado: {len(df):,} registros\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Arquivo '{csv_filename}' n√£o encontrado.\")\n",
    "        return\n",
    "\n",
    "    # üîç QUERY SQL PARA VALIDA√á√ÉO\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üîç QUERY SQL PARA VALIDA√á√ÉO DOS DADOS:\")\n",
    "    print(\"=\"*70)\n",
    "    sql_validacao = \"\"\"\n",
    "-- Leitos totais por UF (√∫ltimos registros)\n",
    "WITH ultimos_registros AS (\n",
    "    SELECT DISTINCT ON (cnes) *\n",
    "    FROM hospitais_leitos\n",
    "    ORDER BY cnes, data_competencia_info DESC\n",
    ")\n",
    "SELECT\n",
    "    uf_hospital,\n",
    "    SUM(leitos_geral) as total_leitos_geral,\n",
    "    SUM(leitos_sus) as total_leitos_sus,\n",
    "    SUM(uti_total) as total_uti,\n",
    "    SUM(uti_sus_total) as total_uti_sus,\n",
    "    COUNT(*) as num_hospitais\n",
    "FROM ultimos_registros\n",
    "GROUP BY uf_hospital\n",
    "ORDER BY total_leitos_geral DESC;\n",
    "\"\"\"\n",
    "    print(sql_validacao)\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    # Agregar por UF\n",
    "    df['uf_hospital'] = df['uf_hospital'].str.strip().str.upper()\n",
    "\n",
    "    leitos_por_uf = df.groupby('uf_hospital').agg({\n",
    "        'leitos_geral': 'sum',\n",
    "        'leitos_sus': 'sum',\n",
    "        'uti_total': 'sum',\n",
    "        'uti_sus_total': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Adicionar popula√ß√£o\n",
    "    leitos_por_uf['populacao'] = leitos_por_uf['uf_hospital'].map(POPULACAO_UF)\n",
    "    leitos_por_uf = leitos_por_uf.dropna(subset=['populacao'])\n",
    "\n",
    "    # Calcular leitos per capita (por 1.000 habitantes)\n",
    "    leitos_por_uf['leitos_geral_per_capita'] = (leitos_por_uf['leitos_geral'] / leitos_por_uf['populacao']) * 1000\n",
    "    leitos_por_uf['leitos_sus_per_capita'] = (leitos_por_uf['leitos_sus'] / leitos_por_uf['populacao']) * 1000\n",
    "    leitos_por_uf['uti_total_per_capita'] = (leitos_por_uf['uti_total'] / leitos_por_uf['populacao']) * 1000\n",
    "    leitos_por_uf['uti_sus_per_capita'] = (leitos_por_uf['uti_sus_total'] / leitos_por_uf['populacao']) * 1000\n",
    "\n",
    "    # Adicionar regi√£o\n",
    "    leitos_por_uf['regiao'] = leitos_por_uf['uf_hospital'].map(UF_PARA_REGIAO)\n",
    "\n",
    "    # Ordenar\n",
    "    leitos_por_uf = leitos_por_uf.sort_values('leitos_geral_per_capita', ascending=True)\n",
    "\n",
    "    print(\"üìä Dados calculados (amostra):\")\n",
    "    print(leitos_por_uf[['uf_hospital', 'leitos_geral', 'populacao', 'leitos_geral_per_capita']].tail(10).to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # Gr√°fico de barras horizontais\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=leitos_por_uf['uf_hospital'],\n",
    "        x=leitos_por_uf['leitos_geral_per_capita'],\n",
    "        name='Leitos Gerais',\n",
    "        orientation='h',\n",
    "        marker_color='#e74c3c'\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=leitos_por_uf['uf_hospital'],\n",
    "        x=leitos_por_uf['leitos_sus_per_capita'],\n",
    "        name='Leitos SUS',\n",
    "        orientation='h',\n",
    "        marker_color='#3498db'\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='üë• Leitos por 1.000 Habitantes - Por Estado',\n",
    "        xaxis_title='Leitos por 1.000 habitantes',\n",
    "        yaxis_title='Estado',\n",
    "        barmode='group',\n",
    "        template='plotly_white',\n",
    "        height=800,\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    # --- Exibe o gr√°fico em vez de salvar como HTML ---\n",
    "    fig.show()\n",
    "    print(f\"‚úÖ Gr√°fico de Leitos per Capita exibido acima.\")\n",
    "\n",
    "\n",
    "    # --- Gr√°fico 2: M√©dia por regi√£o com desvio padr√£o ---\n",
    "    print(f\"\\nüìç Gerando gr√°fico de m√©dia por regi√£o...\")\n",
    "\n",
    "    media_por_regiao = leitos_por_uf.groupby('regiao').agg({\n",
    "        'leitos_geral_per_capita': ['mean', 'std'],\n",
    "        'leitos_geral': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    media_por_regiao.columns = ['regiao', 'media', 'desvio', 'total']\n",
    "\n",
    "    fig_regional = go.Figure()\n",
    "\n",
    "    fig_regional.add_trace(go.Bar(\n",
    "        x=media_por_regiao['regiao'],\n",
    "        y=media_por_regiao['media'],\n",
    "        error_y=dict(type='data', array=media_por_regiao['desvio']),\n",
    "        marker_color=['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6'],\n",
    "        text=media_por_regiao['media'].round(2),\n",
    "        textposition='outside'\n",
    "    ))\n",
    "\n",
    "    fig_regional.update_layout(\n",
    "        title='üìç M√©dia de Leitos per Capita por Regi√£o (com desvio padr√£o)',\n",
    "        xaxis_title='Regi√£o',\n",
    "        yaxis_title='Leitos por 1.000 habitantes (m√©dia)',\n",
    "        template='plotly_white',\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    # --- Exibe o gr√°fico regional em vez de salvar como HTML ---\n",
    "    fig_regional.show()\n",
    "    print(f\"‚úÖ Gr√°fico regional exibido acima.\")\n",
    "\n",
    "\n",
    "    # Estat√≠sticas\n",
    "    print(f\"\\nüìä Estat√≠sticas Nacionais:\")\n",
    "    print(f\"   M√©dia: {leitos_por_uf['leitos_geral_per_capita'].mean():.2f} leitos/1000 hab\")\n",
    "    print(f\"   Mediana: {leitos_por_uf['leitos_geral_per_capita'].median():.2f} leitos/1000 hab\")\n",
    "    print(f\"\\nüèÜ Top 3 Estados:\")\n",
    "    for i, row in leitos_por_uf.tail(3).iterrows():\n",
    "        print(f\"   {row['uf_hospital']}: {row['leitos_geral_per_capita']:.2f} leitos/1000 hab\")\n",
    "\n",
    "    print(f\"\\n‚ö†Ô∏è  Bottom 3 Estados:\")\n",
    "    for i, row in leitos_por_uf.head(3).iterrows():\n",
    "        print(f\"   {row['uf_hospital']}: {row['leitos_geral_per_capita']:.2f} leitos/1000 hab\\n\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. PROPOR√á√ÉO SUS vs PRIVADO\n",
    "# ==============================================================================\n",
    "\n",
    "def analise_proporcao_sus(csv_filename='last.csv'):\n",
    "    \"\"\"\n",
    "    Analisa a propor√ß√£o de leitos SUS vs privados.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üè• AN√ÅLISE 2: PROPOR√á√ÉO SUS vs PRIVADO\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    print(f\"üìÇ Carregando arquivo '{csv_filename}'...\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_filename, encoding='utf-8')\n",
    "        print(f\"‚úÖ Carregado: {len(df):,} registros\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Arquivo '{csv_filename}' n√£o encontrado.\")\n",
    "        return\n",
    "\n",
    "    # üîç QUERY SQL PARA VALIDA√á√ÉO\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üîç QUERY SQL PARA VALIDA√á√ÉO DOS DADOS:\")\n",
    "    print(\"=\"*70)\n",
    "    sql_validacao = \"\"\"\n",
    "-- Propor√ß√£o SUS vs Privado por UF\n",
    "WITH ultimos_registros AS (\n",
    "    SELECT DISTINCT ON (cnes) *\n",
    "    FROM hospitais_leitos\n",
    "    ORDER BY cnes, data_competencia_info DESC\n",
    ")\n",
    "SELECT\n",
    "    uf_hospital,\n",
    "    SUM(leitos_geral) as total_leitos,\n",
    "    SUM(leitos_sus) as total_sus,\n",
    "    SUM(leitos_geral) - SUM(leitos_sus) as total_privado,\n",
    "    ROUND((SUM(leitos_sus)::numeric / NULLIF(SUM(leitos_geral), 0) * 100), 2) as percentual_sus,\n",
    "    SUM(uti_total) as total_uti,\n",
    "    SUM(uti_sus_total) as total_uti_sus,\n",
    "    ROUND((SUM(uti_sus_total)::numeric / NULLIF(SUM(uti_total), 0) * 100), 2) as percentual_uti_sus\n",
    "FROM ultimos_registros\n",
    "GROUP BY uf_hospital\n",
    "ORDER BY percentual_sus DESC;\n",
    "\"\"\"\n",
    "    print(sql_validacao)\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    # Agregar por UF\n",
    "    df['uf_hospital'] = df['uf_hospital'].str.strip().str.upper()\n",
    "\n",
    "    proporcao_uf = df.groupby('uf_hospital').agg({\n",
    "        'leitos_geral': 'sum',\n",
    "        'leitos_sus': 'sum',\n",
    "        'uti_total': 'sum',\n",
    "        'uti_sus_total': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Calcular propor√ß√µes\n",
    "    proporcao_uf['proporcao_sus_geral'] = (proporcao_uf['leitos_sus'] / proporcao_uf['leitos_geral']) * 100\n",
    "    proporcao_uf['proporcao_sus_uti'] = (proporcao_uf['uti_sus_total'] / proporcao_uf['uti_total']) * 100\n",
    "    proporcao_uf['leitos_privados'] = proporcao_uf['leitos_geral'] - proporcao_uf['leitos_sus']\n",
    "    proporcao_uf['uti_privada'] = proporcao_uf['uti_total'] - proporcao_uf['uti_sus_total']\n",
    "\n",
    "    # Adicionar regi√£o\n",
    "    proporcao_uf['regiao'] = proporcao_uf['uf_hospital'].map(UF_PARA_REGIAO)\n",
    "\n",
    "    # Ordenar\n",
    "    proporcao_uf = proporcao_uf.sort_values('proporcao_sus_geral', ascending=False)\n",
    "\n",
    "    print(\"üìä Dados calculados (amostra):\")\n",
    "    print(proporcao_uf[['uf_hospital', 'leitos_geral', 'leitos_sus', 'proporcao_sus_geral']].head(10).to_string(index=False))\n",
    "    print()\n",
    "\n",
    "    # Gr√°fico: Percentual SUS\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=proporcao_uf['uf_hospital'],\n",
    "        y=proporcao_uf['proporcao_sus_geral'],\n",
    "        name='% Leitos SUS',\n",
    "        marker_color='#2ecc71'\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=proporcao_uf['uf_hospital'],\n",
    "        y=proporcao_uf['proporcao_sus_uti'],\n",
    "        name='% UTI SUS',\n",
    "        marker_color='#f39c12'\n",
    "    ))\n",
    "\n",
    "    fig.add_hline(y=50, line_dash=\"dash\", line_color=\"red\",\n",
    "                   annotation_text=\"50%\", annotation_position=\"right\")\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='üìä Percentual de Leitos SUS por Estado',\n",
    "        xaxis_title='Estado',\n",
    "        yaxis_title='Percentual (%)',\n",
    "        barmode='group',\n",
    "        template='plotly_white',\n",
    "        height=600\n",
    "    )\n",
    "\n",
    "    # --- Exibe o gr√°fico em vez de salvar como HTML ---\n",
    "    fig.show()\n",
    "    print(f\"‚úÖ Gr√°fico de Percentual SUS exibido acima.\")\n",
    "\n",
    "    # Estat√≠sticas\n",
    "    print(f\"\\nüìä Estat√≠sticas Nacionais:\")\n",
    "    total_geral = proporcao_uf['leitos_geral'].sum()\n",
    "    total_sus = proporcao_uf['leitos_sus'].sum()\n",
    "    print(f\"   Propor√ß√£o SUS (Leitos Gerais): {(total_sus/total_geral)*100:.1f}%\")\n",
    "\n",
    "    total_uti = proporcao_uf['uti_total'].sum()\n",
    "    total_uti_sus = proporcao_uf['uti_sus_total'].sum()\n",
    "    print(f\"   Propor√ß√£o SUS (UTI): {(total_uti_sus/total_uti)*100:.1f}%\\n\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# FUN√á√ÉO PRINCIPAL\n",
    "# ==============================================================================\n",
    "\n",
    "def gerar_todas_analises(csv_completo='hospitais_leitos_completo.csv',\n",
    "                         csv_ultimo='last.csv'):\n",
    "    \"\"\"\n",
    "    Executa todas as an√°lises recomendadas.\n",
    "\n",
    "    Args:\n",
    "        csv_completo: CSV com todos os registros hist√≥ricos (para an√°lise temporal)\n",
    "        csv_ultimo: CSV com √∫ltimos registros por hospital (para an√°lises atuais)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üöÄ GERADOR DE AN√ÅLISES AVAN√áADAS DE LEITOS HOSPITALARES\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # An√°lise 1: Leitos per capita\n",
    "    analise_leitos_per_capita(csv_ultimo)\n",
    "\n",
    "    # An√°lise 2: Propor√ß√£o SUS\n",
    "    analise_proporcao_sus(csv_ultimo)\n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ TODAS AS AN√ÅLISES CONCLU√çDAS!\")\n",
    "    print(\"=\"*70)\n",
    "    # Removed the output directory and file names as the plots are now displayed directly\n",
    "    print(\"\\nüí° Os gr√°ficos foram exibidos acima.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# EXECU√á√ÉO\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    gerar_todas_analises(\n",
    "        csv_completo='hospitais_leitos_completo.csv',  # Todos os registros\n",
    "        csv_ultimo='hospitais_leitos_latest.csv'  # √öltimos registros por hospital\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
